{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# This notebook creates the datasets for pretraining, training and testing the ANNABELL model using the NYC dataset derived from SQuAD.",
   "id": "56648ebe77800a19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T06:44:27.495233Z",
     "start_time": "2025-11-05T06:44:26.430683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "from dataset_processing import AnnabellCommandGenerator, \\\n",
    "    DatasetPreProcessor, merge_categories, select_pretraining_data, write_pretraining_file, \\\n",
    "    write_testing_file, write_training_file\n",
    "\n",
    "from config.global_config import GlobalConfig\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "global_config = GlobalConfig()\n",
    "percentage_of_pretraining_samples = global_config.percentage_of_pre_training_samples()\n",
    "maximum_number_of_words = global_config.maximum_number_of_words()\n",
    "maximum_word_length = global_config.maximum_word_length()\n",
    "dataset_filepath = global_config.prepared_dataset_filepath()\n",
    "categorised_questions_filepath = global_config.categorised_questions_filepath()\n",
    "categorised_sentences_filepath = global_config.categorised_statements_filepath()\n",
    "pretraining_filepath = global_config.pre_training_filepath()\n",
    "training_filepath = global_config.training_filepath()\n",
    "testing_filepath = global_config.testing_filepath()\n",
    "pretraining_validation_testing_filepath = global_config.pretraining_validation_testing_filepath()\n",
    "logs_directory = global_config.log_archive_directory()"
   ],
   "id": "ada0da2e81534195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 06:44:27,477 - root - INFO - Logging initialized. Log file: /home/chris/logs/cognitive_language_model_logs/run_20251105_064427.log\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T06:44:27.506011Z",
     "start_time": "2025-11-05T06:44:27.502757Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_filepath",
   "id": "4a865dbf5c76f83e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chris/gdrive/work/annabell/experiments/data/response_formatted_20250924_174653.jsonl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T06:42:09.372733Z",
     "start_time": "2025-11-05T06:42:07.102255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasetPreProcessor = DatasetPreProcessor(global_config.prepared_dataset_filepath(),\n",
    "                                          global_config.maximum_number_of_words(), global_config.maximum_word_length())\n",
    "datasetPreProcessor.preprocess_data()\n",
    "nyc_squad_df = datasetPreProcessor.dataset\n",
    "nyc_squad_df = merge_categories(nyc_squad_df, global_config.categorised_questions_filepath(),\n",
    "                                global_config.categorised_statements_filepath())\n",
    "nyc_dataframe = select_pretraining_data(nyc_squad_df, global_config.percentage_of_pre_training_samples())\n",
    "nyc_dataframe"
   ],
   "id": "8f1a827fb43014c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/dataset_processing.py:277: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  categorised_sentences_df = pd.read_json(categorised_sentences_filepath, lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m datasetPreProcessor.preprocess_data()\n\u001B[32m      4\u001B[39m nyc_squad_df = datasetPreProcessor.dataset\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m nyc_squad_df = \u001B[43mmerge_categories\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnyc_squad_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcategorised_questions_filepath\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m                                \u001B[49m\u001B[43mglobal_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcategorised_statements_filepath\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m nyc_dataframe = select_pretraining_data(nyc_squad_df, global_config.percentage_of_pre_training_samples())\n\u001B[32m      8\u001B[39m nyc_dataframe\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Training-and-evaluating-cognitive-language-models/dataset_processing.py:277\u001B[39m, in \u001B[36mmerge_categories\u001B[39m\u001B[34m(the_df, categorised_questions_filepath, categorised_sentences_filepath)\u001B[39m\n\u001B[32m    271\u001B[39m categorised_questions_df = categorised_questions_df.rename(\n\u001B[32m    272\u001B[39m     columns={\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mquestion_category\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m    273\u001B[39m )\n\u001B[32m    274\u001B[39m the_df = the_df.merge(\n\u001B[32m    275\u001B[39m     categorised_questions_df[[\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mquestion_category\u001B[39m\u001B[33m\"\u001B[39m]], on=\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m, how=\u001B[33m\"\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    276\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m categorised_sentences_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcategorised_sentences_filepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlines\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m categorised_sentences_df = categorised_sentences_df.rename(\n\u001B[32m    279\u001B[39m     columns={\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msentence_category\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m    280\u001B[39m )\n\u001B[32m    281\u001B[39m categorised_sentences_df[\u001B[33m\"\u001B[39m\u001B[33msentence_category\u001B[39m\u001B[33m\"\u001B[39m].value_counts()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001B[39m, in \u001B[36mread_json\u001B[39m\u001B[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001B[39m\n\u001B[32m    813\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m json_reader\n\u001B[32m    814\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m815\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjson_reader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1023\u001B[39m, in \u001B[36mJsonReader.read\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1021\u001B[39m         data = ensure_str(\u001B[38;5;28mself\u001B[39m.data)\n\u001B[32m   1022\u001B[39m         data_lines = data.split(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1023\u001B[39m         obj = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_object_parser\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_combine_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_lines\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1025\u001B[39m     obj = \u001B[38;5;28mself\u001B[39m._get_object_parser(\u001B[38;5;28mself\u001B[39m.data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1051\u001B[39m, in \u001B[36mJsonReader._get_object_parser\u001B[39m\u001B[34m(self, json)\u001B[39m\n\u001B[32m   1049\u001B[39m obj = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1050\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m typ == \u001B[33m\"\u001B[39m\u001B[33mframe\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m     obj = \u001B[43mFrameParser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m typ == \u001B[33m\"\u001B[39m\u001B[33mseries\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, \u001B[38;5;28mbool\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1187\u001B[39m, in \u001B[36mParser.parse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1185\u001B[39m \u001B[38;5;129m@final\u001B[39m\n\u001B[32m   1186\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparse\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1187\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1189\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1190\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1403\u001B[39m, in \u001B[36mFrameParser._parse\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1399\u001B[39m orient = \u001B[38;5;28mself\u001B[39m.orient\n\u001B[32m   1401\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m orient == \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1402\u001B[39m     \u001B[38;5;28mself\u001B[39m.obj = DataFrame(\n\u001B[32m-> \u001B[39m\u001B[32m1403\u001B[39m         \u001B[43mujson_loads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprecise_float\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprecise_float\u001B[49m\u001B[43m)\u001B[49m, dtype=\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1404\u001B[39m     )\n\u001B[32m   1405\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m orient == \u001B[33m\"\u001B[39m\u001B[33msplit\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1406\u001B[39m     decoded = {\n\u001B[32m   1407\u001B[39m         \u001B[38;5;28mstr\u001B[39m(k): v\n\u001B[32m   1408\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m ujson_loads(json, precise_float=\u001B[38;5;28mself\u001B[39m.precise_float).items()\n\u001B[32m   1409\u001B[39m     }\n",
      "\u001B[31mValueError\u001B[39m: Expected object or value"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### create the pretraining data\n",
    "Select the rows where the pretraining is true, generate a set of commands for each row and save to a file."
   ],
   "id": "1a14d58f1bfc43ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T08:16:33.665467Z",
     "start_time": "2025-10-26T08:16:33.487887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#add a new column to the dataframe with the created list of commands\n",
    "nyc_squad_df[\"created_commands\"] = nyc_squad_df.apply(\n",
    "    lambda row: AnnabellCommandGenerator(\n",
    "        row['id'], row['response_declarative_sentence_formatted'], row['response_question_formatted'],\n",
    "        row['response_answer_formatted']\n",
    "    ).create_list_of_commands(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "nyc_squad_training_df = nyc_squad_df[nyc_squad_df[\"is_pretraining\"] == False]\n",
    "nyc_squad_pretraining_df = nyc_squad_df[nyc_squad_df[\"is_pretraining\"] == True]\n",
    "#save the final dataframe as a JSON lines file\n",
    "nyc_squad_df.to_json(global_config.prepared_dataset_with_commands_filepath(), orient=\"records\",\n",
    "                     lines=True)"
   ],
   "id": "11c6634dc7bd8a5c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Write the files containing the commands to perform pretraining, training and testing on ANNABELL",
   "id": "8a57ccbb2a8bae17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T08:17:59.562710Z",
     "start_time": "2025-10-26T08:17:59.553390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "write_pretraining_file(pretraining_filepath, nyc_squad_pretraining_df)\n",
    "write_training_file(training_filepath, nyc_squad_training_df)\n",
    "write_testing_file(testing_filepath, nyc_squad_training_df)\n",
    "write_testing_file(pretraining_validation_testing_filepath, nyc_squad_pretraining_df)"
   ],
   "id": "2a7b1a7161c94a19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/15/pre_training/nyc_squad_pretraining_commands_20251026_091610.txt\n",
      "Number of reward lines: 42\n",
      "Number of commands: 773\n",
      "#id: 56ce32e7aab44d1400b88551\n",
      "there are 469 station -s operate -d by the New-York-City-Subway\n",
      "\n",
      "\n",
      "? how many station -s are operate -d by the\n",
      "New-York-City-Subway\n",
      ".sctx ? how many station -s are operate -d by the\n",
      ".wg many\n",
      ".wg station\n",
      ".wg operate\n",
      ".sctx New-York-City-Subway\n",
      ".wg New-York-City-Subway\n",
      ".ph there are 469 station -s operate -d by the New-York-City-Subway\n",
      ".wg 469\n",
      ".rw\n",
      "\n",
      "\n",
      "#id: 56ce345caab44d1400b88584\n",
      "Giovanni da Verrazzano call -ed the area Nouvelle-Angouleme when he\n",
      "stake -d a claim on it\n",
      "file written: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/15/training/nyc_squad_training_commands_20251026_091610.txt\n",
      "Number of commands: 2130\n",
      "First 20 lines:\n",
      "#id: 56ce304daab44d1400b8850e\n",
      "the city in the United-States with the high -est populate -ion is New-York\n",
      "\n",
      "#id: 56ce304daab44d1400b8850f\n",
      "the United-Nations is base -d in New-York\n",
      "\n",
      "#id: 56ce304daab44d1400b88510\n",
      "New-York has been call -ed the culture -al capital of the world\n",
      "\n",
      "#id: 56ce304daab44d1400b88511\n",
      "New-York is the American city that welcome -s the large -st number of legal immigrant -s\n",
      "\n",
      "#id: 56cf5d41aab44d1400b89130\n",
      "the major gateway for immigrate -ion has been New-York-City\n",
      "\n",
      "#id: 56cf5d41aab44d1400b89131\n",
      "the most populate -d city in the United-States is New-York-City\n",
      "\n",
      "#id: 56ce3124aab44d1400b8852a\n",
      "New-York-City is comprise -d of five borough -s\n",
      "file written: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/15/testing/nyc_squad_testing_commands_20251026_091610.txt\n",
      "Number of commands: 3550\n",
      "First 20 lines:\n",
      "#id: 56ce304daab44d1400b8850e\n",
      "? what city in the United-States has the high -est populate -ion\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56ce304daab44d1400b8850f\n",
      "? in what city is the United-Nations base -d\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56ce304daab44d1400b88510\n",
      "? what city has been call -ed the culture -al capital of the world\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56ce304daab44d1400b88511\n",
      "? what American city welcome -s the large -st number of legal immigrant -s\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "file written: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/15/testing/nyc_squad_pretraining_validation_testing_commands_20251026_091610.txt\n",
      "Number of commands: 210\n",
      "First 20 lines:\n",
      "#id: 56ce32e7aab44d1400b88551\n",
      "? how many station -s are operate -d by the New-York-City-Subway\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56ce345caab44d1400b88584\n",
      "? what did Giovanni da Verrazzano call the area when he stake -d claim on it\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56ce3569aab44d1400b885ae\n",
      "? what did Henry-Hudson call the river that is now call -ed the Hudson-River\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n",
      "#id: 56cfab96234ae51400d9be44\n",
      "? in what year was the land between Cape-Cod and Delaware-Bay claim -ed by the Dutch\n",
      ".x\n",
      "#END OF TESTING SAMPLE\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate the command line instructions for running the experiments",
   "id": "facd6d44c0854feb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T08:19:12.140271Z",
     "start_time": "2025-10-26T08:19:12.132623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Copy the data files to the docker shared data directory for processing\n",
    "shutil.copy(pretraining_filepath, global_config.docker_pre_training_directory())\n",
    "shutil.copy(training_filepath, global_config.docker_training_directory())\n",
    "shutil.copy(testing_filepath, global_config.docker_testing_directory())\n",
    "shutil.copy(pretraining_validation_testing_filepath, global_config.docker_testing_directory())\n",
    "\n",
    "print(\"Files copied to docker shared data directory.\")"
   ],
   "id": "a061d26e38998d74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied to docker shared data directory.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T08:20:22.735179Z",
     "start_time": "2025-10-26T08:20:22.731688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create the pre-training command\n",
    "print(\n",
    "    f'docker compose run --remove-orphans --entrypoint ./pre_train_annabell_squad_nyc.sh app data/pre-training/logfile_nyc_squad_pretraining_commands.txt data/pre-training/{global_config.pre_training_filename()} data/pre-training/{global_config.pre_training_filename().replace(\".txt\", \".dat\")}')"
   ],
   "id": "53673eb5619e4a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker compose run --remove-orphans --entrypoint ./pre_train_annabell_squad_nyc.sh app data/pre-training/logfile_nyc_squad_pretraining_commands.txt data/pre-training/nyc_squad_pretraining_commands_20251026_091610.txt data/pre-training/nyc_squad_pretraining_commands_20251026_091610.dat\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T16:37:15.261709Z",
     "start_time": "2025-10-22T16:37:15.059661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#copy the pre-trained weights to the pre-training directory\n",
    "weights_filename = global_config.pre_training_filename().replace(\".txt\", \".dat\")\n",
    "source_path = os.path.join(global_config.get_docker_data_directory(), \"pre-training\", weights_filename)\n",
    "destination_path = os.path.join(global_config.get_docker_data_directory(), weights_filename)\n",
    "\n",
    "try:\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    print(\"copied: \" + source_path + \" to: \" + destination_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source file not found at {source_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "#move the pre-training logfile to the pre-training directory\n",
    "source_pattern = os.path.join(global_config.get_docker_data_directory(), \"pre-training\",\n",
    "                              \"logfile_nyc_squad_pretraining_commands*\")\n",
    "destination_dir = logs_directory\n",
    "\n",
    "log_files = glob.glob(source_pattern)\n",
    "if not log_files:\n",
    "    print(f\"No log files found matching pattern: {source_pattern}\")\n",
    "\n",
    "for file_path in log_files:\n",
    "    try:\n",
    "        shutil.move(file_path, destination_dir)\n",
    "        print(f\"moved: {file_path} to: {destination_dir}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Log file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while moving {file_path}: {e}\")"
   ],
   "id": "e74196a38431a761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/pre-training/nyc_squad_pretraining_commands_20251022_085512.dat to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/pre_training/nyc_squad_pretraining_commands_20251022_085512.dat\n",
      "moved: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/pre-training/logfile_nyc_squad_pretraining_commands_2025-10-22_06-56-50.txt to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/logs\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T16:37:55.097416Z",
     "start_time": "2025-10-22T16:37:55.094900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#run the pretraining validation testing\n",
    "print(\n",
    "    f'docker compose run --remove-orphans --entrypoint ./test_annabell_squad.sh app data/testing/logfile_nyc_squad_pretraining_validation_testing_commands.txt data/pre-training/{global_config.pre_training_filename().replace(\".txt\", \".dat\")} data/testing/{global_config.pre_training_validation_testing_filename()}')"
   ],
   "id": "cd94dbb51fefd2fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker compose run --remove-orphans --entrypoint ./test_annabell_squad.sh app data/testing/logfile_nyc_squad_pretraining_validation_testing_commands.txt data/pre-training/nyc_squad_pretraining_commands_20251022_085512.dat data/testing/nyc_squad_pretraining_validation_testing_commands_20251022_085512.txt\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T16:56:11.845295Z",
     "start_time": "2025-10-22T16:56:11.837630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Copy the testing logs back to the experiment directory\n",
    "source_pattern = os.path.join(global_config.get_docker_data_directory(), \"testing\",\n",
    "                              \"logfile_nyc_squad_pretraining_validation_testing_commands*\")\n",
    "destination_dir = logs_directory\n",
    "# Find all files matching the pattern\n",
    "log_files = glob.glob(source_pattern)\n",
    "# Copy each found file to the destination directory\n",
    "for file_path in log_files:\n",
    "    try:\n",
    "        shutil.copy(file_path, destination_dir)\n",
    "        print(f\"copied: {file_path} to: {destination_dir}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "id": "8ca1e195442ff549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/testing/logfile_nyc_squad_pretraining_validation_testing_commands_2025-10-22_16-38-14.txt to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/logs\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### perform the testing using the \"test annabell\" notebook",
   "id": "954ccacd319f5b7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:45:45.781667Z",
     "start_time": "2025-10-23T05:45:45.778990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print the command to run the training\n",
    "print(\n",
    "    f'docker compose run --remove-orphans --entrypoint ./train_annabell_squad.sh app data/training/logfile_nyc_squad_training_commands.txt data/pre-training/{global_config.pre_training_filename().replace(\".txt\", \".dat\")} data/training/{global_config.training_filename().replace(\".txt\", \".dat\")} data/training/{global_config.training_filename()}')"
   ],
   "id": "6db702604cc58727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker compose run --remove-orphans --entrypoint ./train_annabell_squad.sh app data/training/logfile_nyc_squad_training_commands.txt data/pre-training/nyc_squad_pretraining_commands_20251022_085512.dat data/training/nyc_squad_training_commands_20251022_085512.dat data/training/nyc_squad_training_commands_20251022_085512.txt\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:48:03.663031Z",
     "start_time": "2025-10-23T05:48:03.426372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#copy the training weights back to the experiment directory\n",
    "source_path = os.path.join(global_config.get_docker_data_directory(), \"training\",\n",
    "                           global_config.training_filename().replace(\".txt\", \".dat\"))\n",
    "destination_path = os.path.join(global_config.training_directory(),\n",
    "                                global_config.training_filename().replace(\".txt\", \".dat\"))\n",
    "try:\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    print(\"copied: \" + source_path + \" to: \" + destination_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source file not found at {source_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "#copy the training logfile to the logs directory\n",
    "source_pattern = os.path.join(global_config.get_docker_data_directory(), \"training\",\n",
    "                              \"logfile_nyc_squad_training_commands*\")\n",
    "destination_dir = logs_directory\n",
    "# Find all files matching the pattern\n",
    "log_files = glob.glob(source_pattern)\n",
    "# Move each found file to the destination directory\n",
    "for file_path in log_files:\n",
    "    try:\n",
    "        shutil.move(file_path, destination_dir)\n",
    "        print(f\"moved: {file_path} to: {destination_dir}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Log file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while moving {file_path}: {e}\")"
   ],
   "id": "a205087674b0dec9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/training/nyc_squad_training_commands_20251022_085512.dat to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/training/nyc_squad_training_commands_20251022_085512.dat\n",
      "moved: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/training/logfile_nyc_squad_training_commands_2025-10-23_05-46-23.txt to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/logs\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:48:18.215396Z",
     "start_time": "2025-10-23T05:48:18.212957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print the command to run the testing\n",
    "print(\"#run the testing\")\n",
    "print(\n",
    "    f'docker compose run --remove-orphans --entrypoint ./test_annabell_squad.sh app data/testing/logfile_nyc_squad_testing_commands.txt data/training/{training_filename.replace(\".txt\", \".dat\")} data/testing/{global_config.testing_filename()}')"
   ],
   "id": "cec14deb7ac434d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#run the testing\n",
      "docker compose run --remove-orphans --entrypoint ./test_annabell_squad.sh app data/testing/logfile_nyc_squad_testing_commands.txt data/training/nyc_squad_training_commands_20251022_085512.dat data/testing/nyc_squad_testing_commands_20251022_085512.txt\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T06:54:00.402071Z",
     "start_time": "2025-10-23T06:54:00.392615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Copy the testing logs back to the experiment directory\n",
    "source_pattern = os.path.join(global_config.get_docker_data_directory(), \"testing\",\n",
    "                              \"logfile_nyc_squad_testing_commands*\")\n",
    "destination_dir = logs_directory\n",
    "# Find all files matching the pattern\n",
    "log_files = glob.glob(source_pattern)\n",
    "# Copy each found file to the destination directory\n",
    "for file_path in log_files:\n",
    "    try:\n",
    "        shutil.copy(file_path, destination_dir)\n",
    "        print(f\"copied: {file_path} to: {destination_dir}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source file not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "id": "8d5eb4a819ec2ee8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied: /Users/chris/PycharmProjects/Training-and-evaluating-cognitive-language-models/docker/shared_data/testing/logfile_nyc_squad_testing_commands_2025-10-23_05-48-29.txt to: /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/14/logs\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run the testing notebook to evaluate the results",
   "id": "d70e820990ad023b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
