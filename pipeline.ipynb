{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from config.global_config import GlobalConfig\n",
    "from pipeline import Pipeline\n",
    "from training import AnnabellPreTrainingTestingRunner\n",
    "\n",
    "global_config = GlobalConfig()\n",
    "\n",
    "pipeline = Pipeline(prepared_dataset_filepath=global_config.prepared_dataset_with_commands_filepath())\n",
    "pipeline.load_prepared_dataset()\n",
    "pipeline.run_pre_training_evaluation_testing()\n",
    "pipeline.run_evaluate_pre_training_results()\n",
    "#runner = AnnabellPreTrainingTestingRunner(pipeline.dataset_processor)\n",
    "#runner.move_annabell_logfile_to_gdrive()"
   ],
   "id": "16978073ec3a4ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# todo\n",
    "\n",
    "1. only add prw if the wg is not the last one.\n",
    "\n",
    ".ph a golden statue of the_Virgin_Mary sit on top of the_Main_Building\n",
    ".wg a golden statue\n",
    ".prw\n",
    ".wg of the_Virgin_Mary\n",
    ".prw\n",
    ".rw\n",
    "\n",
    "2. why .ph is written twice?\n",
    "#id: 5733be284776f41900661180\n",
    "the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    "the_Main_Building\n",
    "\n",
    "\n",
    "? the Basilica of the sacred heart at Notre_Dame be\n",
    "beside to which structure\n",
    ".sctx ? the Basilica of the sacred heart at Notre_Dame be\n",
    ".wg Basilica\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".wg Notre_Dame\n",
    ".sctx beside to which structure\n",
    ".wg beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".sctx the_Main_Building\n",
    ".wg the_Main_Building\n",
    ".rw\n",
    "\n",
    "3. associations only built up to max words -1 in the below example \"of prayer\" wg is not built on phrase ingestion\n",
    "#id: 5733be284776f41900661181\n",
    "the Grotto at Notre_Dame be a marian place of prayer\n",
    "and reflection\n",
    "\n",
    "\n",
    "? what be the Grotto at Notre_Dame\n",
    ".sctx ? what be the Grotto at Notre_Dame\n",
    ".wg Grotto\n",
    ".wg Notre_Dame\n",
    ".ph the Grotto at Notre_Dame be a marian place of prayer\n",
    ".wg a marian place\n",
    ".prw\n",
    ".wg of prayer\n",
    ".prw\n",
    ".sctx and reflection\n",
    ".wg and reflection\n",
    ".rw"
   ],
   "id": "db05045824fbd0c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from training import AnnabellPreTrainingTestingRunner\n",
    "\n",
    "pipeline = Pipeline(prepared_dataset_filepath=global_config.prepared_dataset_with_commands_filepath())\n",
    "pipeline.load_prepared_dataset()\n",
    "runner = AnnabellPreTrainingTestingRunner(pipeline.dataset_processor)"
   ],
   "id": "5204822f7739bd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "\n",
    "def _load_spacy_model(model_name):\n",
    "    \"\"\"Loads a spaCy model, downloading it if necessary.\"\"\"\n",
    "    try:\n",
    "        return spacy.load(model_name)\n",
    "    except OSError:\n",
    "        download(model_name)\n",
    "        return spacy.load(model_name)\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = _load_spacy_model(\"en_core_web_sm\")"
   ],
   "id": "2e83d1be16a1feec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m785.8 kB/s\u001B[0m  \u001B[33m0:00:16\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "--- Token and Dependency Analysis ---\n",
      "TOKEN      LEMMA      DEP        HEAD      \n",
      "----------------------------------------\n",
      "what       what       nsubj      sit       \n",
      "sit        sit        ROOT       sit       \n",
      "on         on         prep       sit       \n",
      "top        top        pobj       on        \n",
      "of         of         prep       top       \n",
      "the        the        det        Main_Building\n",
      "Main_Building main_building pobj       of        \n",
      "at         at         prep       sit       \n",
      "Notre_Dame Notre_Dame pobj       at        \n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "\n",
    "# The sentence (adjusting for the standard space usage)\n",
    "text = \"what sit on top of the Main_Building at Notre_Dame\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"--- Token and Dependency Analysis ---\")\n",
    "print(\"{:<10} {:<10} {:<10} {:<10}\".format(\"TOKEN\", \"LEMMA\", \"DEP\", \"HEAD\"))\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    # DEP: The dependency relation label\n",
    "    # HEAD: The head token of this token\n",
    "    print(\"{:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        token.text, token.lemma_, token.dep_, token.head.text\n",
    "    ))\n",
    "\n"
   ],
   "id": "20622e04cc4014aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:56:16.802755Z",
     "start_time": "2025-11-19T08:56:16.676974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ],
   "id": "7397b91816aba606",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1050] Port 5000 is already in use. Please specify an available port with `displacy.serve(doc, port=port)` or use `auto_select_port=True` to pick an available port automatically.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mspacy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m displacy\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mdisplacy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdep\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Training-and-evaluating-cognitive-language-models/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:105\u001B[39m, in \u001B[36mserve\u001B[39m\u001B[34m(docs, style, page, minify, options, manual, port, host, auto_select_port)\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Serve displaCy visualisation.\u001B[39;00m\n\u001B[32m     89\u001B[39m \n\u001B[32m     90\u001B[39m \u001B[33;03mdocs (list or Doc): Document(s) to visualise.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    101\u001B[39m \u001B[33;03mUSAGE: https://spacy.io/usage/visualizers\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwsgiref\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m simple_server\n\u001B[32m--> \u001B[39m\u001B[32m105\u001B[39m port = \u001B[43mfind_available_port\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauto_select_port\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_in_jupyter():\n\u001B[32m    108\u001B[39m     warnings.warn(Warnings.W011)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Training-and-evaluating-cognitive-language-models/.venv/lib/python3.12/site-packages/spacy/util.py:1884\u001B[39m, in \u001B[36mfind_available_port\u001B[39m\u001B[34m(start, host, auto_select)\u001B[39m\n\u001B[32m   1882\u001B[39m port = start\n\u001B[32m   1883\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m auto_select:\n\u001B[32m-> \u001B[39m\u001B[32m1884\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors.E1050.format(port=port))\n\u001B[32m   1886\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m _is_port_in_use(port, host) \u001B[38;5;129;01mand\u001B[39;00m port < \u001B[32m65535\u001B[39m:\n\u001B[32m   1887\u001B[39m     port += \u001B[32m1\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: [E1050] Port 5000 is already in use. Please specify an available port with `displacy.serve(doc, port=port)` or use `auto_select_port=True` to pick an available port automatically."
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
