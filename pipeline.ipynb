{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T06:50:22.681655Z",
     "start_time": "2025-11-26T06:50:20.081005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config.global_config import GlobalConfig\n",
    "from pipeline import Pipeline\n",
    "from training import AnnabellPreTrainingTestingRunner, AnnabellTestingRunner\n",
    "\n",
    "global_config = GlobalConfig()"
   ],
   "id": "16978073ec3a4ba7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:50:20,083 - root - INFO - Logging initialized. Log file: /Users/chris/logs/cognitive_language_model_logs/run_20251126_065020.log\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T06:32:36.718092Z",
     "start_time": "2025-11-25T06:32:35.789244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pipeline = Pipeline(prepared_dataset_filepath=global_config.prepared_dataset_with_commands_filepath())\n",
    "pipeline.load_prepared_dataset()\n",
    "#pipeline.run_pre_training_evaluation_testing()\n",
    "#pipeline.run_evaluate_pre_training_results()\n",
    "#runner = AnnabellPreTrainingTestingRunner(pipeline.dataset_processor)\n",
    "#runner.move_annabell_logfile_to_gdrive()"
   ],
   "id": "1cb36bbde4c86f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-25 06:32:35,797 - pipeline - INFO - Loading prepared dataset from /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/sandbox/data_sets/nyc_squad_with_pretraining_commands.jsonl...\n",
      "2025-11-25 06:32:36,709 - pipeline - INFO - Prepared dataset loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ecf2913793fedcc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# todo\n",
    "\n",
    "1. only add prw if the wg is not the last one.\n",
    "\n",
    ".ph a golden statue of the_Virgin_Mary sit on top of the_Main_Building\n",
    ".wg a golden statue\n",
    ".prw\n",
    ".wg of the_Virgin_Mary\n",
    ".prw\n",
    ".rw\n",
    "\n",
    "2. why .ph is written twice?\n",
    "#id: 5733be284776f41900661180\n",
    "the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    "the_Main_Building\n",
    "\n",
    "\n",
    "? the Basilica of the sacred heart at Notre_Dame be\n",
    "beside to which structure\n",
    ".sctx ? the Basilica of the sacred heart at Notre_Dame be\n",
    ".wg Basilica\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".wg Notre_Dame\n",
    ".sctx beside to which structure\n",
    ".wg beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".ph the Basilica of the Sacred Heart at Notre_Dame be beside\n",
    ".sctx the_Main_Building\n",
    ".wg the_Main_Building\n",
    ".rw\n",
    "\n",
    "3. associations only built up to max words -1 in the below example \"of prayer\" wg is not built on phrase ingestion\n",
    "#id: 5733be284776f41900661181\n",
    "the Grotto at Notre_Dame be a marian place of prayer\n",
    "and reflection\n",
    "\n",
    "\n",
    "? what be the Grotto at Notre_Dame\n",
    ".sctx ? what be the Grotto at Notre_Dame\n",
    ".wg Grotto\n",
    ".wg Notre_Dame\n",
    ".ph the Grotto at Notre_Dame be a marian place of prayer\n",
    ".wg a marian place\n",
    ".prw\n",
    ".wg of prayer\n",
    ".prw\n",
    ".sctx and reflection\n",
    ".wg and reflection\n",
    ".rw"
   ],
   "id": "db05045824fbd0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T06:50:34.345595Z",
     "start_time": "2025-11-26T06:50:32.846889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training import AnnabellPreTrainingTestingRunner\n",
    "\n",
    "pipeline = Pipeline(prepared_dataset_filepath=global_config.prepared_dataset_with_commands_filepath())\n",
    "pipeline.load_prepared_dataset()\n",
    "runner = AnnabellTestingRunner(pipeline.dataset_processor)\n",
    "\n",
    "pipeline.run_evaluate_training_results()"
   ],
   "id": "5204822f7739bd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:50:32,848 - pipeline - INFO - Loading prepared dataset from /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/sandbox/data_sets/nyc_squad_with_pretraining_commands.jsonl...\n",
      "2025-11-26 06:50:33,759 - pipeline - INFO - Prepared dataset loaded successfully.\n",
      "2025-11-26 06:50:33,760 - pipeline - INFO - Starting evaluation of training results...\n",
      "2025-11-26 06:50:33,762 - testing - INFO - length of log file questions and answers: 2\n",
      "2025-11-26 06:50:33,764 - testing - INFO - number of test samples not found in training data: 0\n",
      "2025-11-26 06:50:33,765 - testing - INFO - test samples not found in training data: [] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test answer embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:50:34,225 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test answer embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "Generating response answer embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:50:34,321 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response answer embeddings: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:50:34,327 - testing - INFO - number of test answers longer than 20 words: 0\n",
      "2025-11-26 06:50:34,328 - testing - INFO - number correct = 0 out of 1\n",
      "2025-11-26 06:50:34,329 - testing - INFO - percentage correct = 0.0 %\n",
      "2025-11-26 06:50:34,332 - testing - INFO - number any word matches = 0 out of 1\n",
      "2025-11-26 06:50:34,332 - testing - INFO - percentage any word matches = 0.0 %\n",
      "2025-11-26 06:50:34,333 - testing - INFO - number of rows with cosine distance less than 0.1: 0\n",
      "2025-11-26 06:50:34,333 - testing - INFO - percentage of total: <bound method AnnabellTestResultsEvaluator.percentage_of_answers_below_cosine_distance_threshold of <testing.AnnabellTestResultsEvaluator object at 0x137ad0830>> %\n",
      "2025-11-26 06:50:34,333 - testing - INFO - number of rows with cosine distance less than 0.1 and any matching answer correct: 0\n",
      "percentage of total: 0.0 %\n",
      "2025-11-26 06:50:34,336 - testing - INFO - total number of test samples in input file: 2\n",
      "results written to /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/sandbox/training/results/test_detailed_results_tsv and /Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work/annabell/experiments/sandbox/training/results/test_summary_results_txt\n",
      "2025-11-26 06:50:34,343 - pipeline - INFO - Evaluation of training results completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:49:24.743552Z",
     "start_time": "2025-11-24T13:49:22.559341Z"
    }
   },
   "cell_type": "code",
   "source": "global_config.prepared_dataset_with_commands_filepath_exists()",
   "id": "899ec8aa687d2a18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T14:17:03.318506Z",
     "start_time": "2025-11-24T14:16:47.872366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from testing import AnnabellPreTrainingTestContext, AnnabellTestResultsEvaluator\n",
    "\n",
    "pipeline = Pipeline(prepared_dataset_filepath=global_config.prepared_dataset_with_commands_filepath())\n",
    "pipeline.load_prepared_dataset()\n",
    "testing_context = AnnabellPreTrainingTestContext(pipeline.datasetPreProcessor)\n",
    "evaluator = AnnabellTestResultsEvaluator(testing_context)\n",
    "evaluator.run()"
   ],
   "id": "dabf53b773b2ee91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:49,897 - pipeline - INFO - Loading prepared dataset from /home/chris/gdrive/work/annabell/experiments/sandbox/data_sets/nyc_squad_with_pretraining_commands.jsonl...\n",
      "2025-11-24 14:16:52,110 - pipeline - INFO - Prepared dataset loaded successfully.\n",
      "2025-11-24 14:16:53,180 - testing - INFO - length of log file questions and answers: 4\n",
      "2025-11-24 14:16:53,182 - testing - INFO - number of test samples not found in training data: 0\n",
      "2025-11-24 14:16:53,183 - testing - INFO - test samples not found in training data: [] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test answer embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:53,951 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test answer embeddings:  67%|██████▋   | 2/3 [00:00<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:54,040 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 14:16:54,129 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test answer embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n",
      "Generating response answer embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:54,222 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 14:16:54,306 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response answer embeddings: 100%|██████████| 3/3 [00:00<00:00, 17.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:54,397 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating response answer embeddings: 100%|██████████| 3/3 [00:00<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:16:58,196 - testing - INFO - number of test answers longer than 20 words: 0\n",
      "2025-11-24 14:16:58,197 - testing - INFO - number correct = 2 out of 3\n",
      "2025-11-24 14:16:58,198 - testing - INFO - percentage correct = 66.66666666666666 %\n",
      "2025-11-24 14:16:58,199 - testing - INFO - number any word matches = 2 out of 3\n",
      "2025-11-24 14:16:58,200 - testing - INFO - percentage any word matches = 66.66666666666666 %\n",
      "2025-11-24 14:16:58,200 - testing - INFO - number of rows with cosine distance less than 0.1: 2\n",
      "2025-11-24 14:16:58,201 - testing - INFO - percentage of total: <bound method AnnabellTestResultsEvaluator.percentage_of_answers_below_cosine_distance_threshold of <testing.AnnabellTestResultsEvaluator object at 0x7ff8ccbdffe0>> %\n",
      "2025-11-24 14:16:58,201 - testing - INFO - number of rows with cosine distance less than 0.1 and any matching answer correct: 2\n",
      "percentage of total: 66.66666666666666 %\n",
      "2025-11-24 14:17:01,168 - testing - INFO - total number of test samples in input file: 4\n",
      "results written to /home/chris/gdrive/work/annabell/experiments/sandbox/pre_training/results/test_detailed_results_tsv and /home/chris/gdrive/work/annabell/experiments/sandbox/pre_training/results/test_summary_results_txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "\n",
    "def _load_spacy_model(model_name):\n",
    "    \"\"\"Loads a spaCy model, downloading it if necessary.\"\"\"\n",
    "    try:\n",
    "        return spacy.load(model_name)\n",
    "    except OSError:\n",
    "        download(model_name)\n",
    "        return spacy.load(model_name)\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = _load_spacy_model(\"en_core_web_sm\")"
   ],
   "id": "2e83d1be16a1feec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m785.8 kB/s\u001B[0m  \u001B[33m0:00:16\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "--- Token and Dependency Analysis ---\n",
      "TOKEN      LEMMA      DEP        HEAD      \n",
      "----------------------------------------\n",
      "what       what       nsubj      sit       \n",
      "sit        sit        ROOT       sit       \n",
      "on         on         prep       sit       \n",
      "top        top        pobj       on        \n",
      "of         of         prep       top       \n",
      "the        the        det        Main_Building\n",
      "Main_Building main_building pobj       of        \n",
      "at         at         prep       sit       \n",
      "Notre_Dame Notre_Dame pobj       at        \n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "\n",
    "# The sentence (adjusting for the standard space usage)\n",
    "text = \"what sit on top of the Main_Building at Notre_Dame\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"--- Token and Dependency Analysis ---\")\n",
    "print(\"{:<10} {:<10} {:<10} {:<10}\".format(\"TOKEN\", \"LEMMA\", \"DEP\", \"HEAD\"))\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    # DEP: The dependency relation label\n",
    "    # HEAD: The head token of this token\n",
    "    print(\"{:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        token.text, token.lemma_, token.dep_, token.head.text\n",
    "    ))\n",
    "\n"
   ],
   "id": "20622e04cc4014aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:56:16.802755Z",
     "start_time": "2025-11-19T08:56:16.676974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ],
   "id": "7397b91816aba606",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1050] Port 5000 is already in use. Please specify an available port with `displacy.serve(doc, port=port)` or use `auto_select_port=True` to pick an available port automatically.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mspacy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m displacy\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mdisplacy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdep\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Training-and-evaluating-cognitive-language-models/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:105\u001B[39m, in \u001B[36mserve\u001B[39m\u001B[34m(docs, style, page, minify, options, manual, port, host, auto_select_port)\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Serve displaCy visualisation.\u001B[39;00m\n\u001B[32m     89\u001B[39m \n\u001B[32m     90\u001B[39m \u001B[33;03mdocs (list or Doc): Document(s) to visualise.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    101\u001B[39m \u001B[33;03mUSAGE: https://spacy.io/usage/visualizers\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwsgiref\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m simple_server\n\u001B[32m--> \u001B[39m\u001B[32m105\u001B[39m port = \u001B[43mfind_available_port\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauto_select_port\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_in_jupyter():\n\u001B[32m    108\u001B[39m     warnings.warn(Warnings.W011)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Training-and-evaluating-cognitive-language-models/.venv/lib/python3.12/site-packages/spacy/util.py:1884\u001B[39m, in \u001B[36mfind_available_port\u001B[39m\u001B[34m(start, host, auto_select)\u001B[39m\n\u001B[32m   1882\u001B[39m port = start\n\u001B[32m   1883\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m auto_select:\n\u001B[32m-> \u001B[39m\u001B[32m1884\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors.E1050.format(port=port))\n\u001B[32m   1886\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m _is_port_in_use(port, host) \u001B[38;5;129;01mand\u001B[39;00m port < \u001B[32m65535\u001B[39m:\n\u001B[32m   1887\u001B[39m     port += \u001B[32m1\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: [E1050] Port 5000 is already in use. Please specify an available port with `displacy.serve(doc, port=port)` or use `auto_select_port=True` to pick an available port automatically."
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
