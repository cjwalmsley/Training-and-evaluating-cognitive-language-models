{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This notebook prepares a prompt that can be used to generate declarative sentences from questions and answers\n",
    "# The model used is Gemini 2.5 pro\n",
    "\n",
    "1. Load the squad dataset\n",
    "2. Filter the dataset by the category \"New_York_City\"\n",
    "3. Write the dataset rows to a dataframe\n",
    "3. Create a prompt that, for each row, using the question, answer pair and statement, will generate a declarative sentence\n",
    "4. The response will be a JSON object with the following fields:\n",
    "\t* \"declarative_sentence\": The declarative sentence generated from the question and answer\n",
    "\t* \"original_question\": The original question\n",
    "\t* \"original_answer\": The original answer"
   ],
   "id": "e6258f3c44dfc53c"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "import json\n",
    "from dataset_processing import load_squad_dataset"
   ],
   "id": "a7a6c911a4d48436"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_dir = \"/Users/chris/Library/CloudStorage/GoogleDrive-cjameswalmsley@gmail.com/My Drive/Shared with Julia/Education/Kent University/PhD/work\"\n",
    "dataset_dir = \"datasets/squad_dataset\"\n",
    "dataset_path = os.path.join(base_dir, dataset_dir)\n",
    "nyc_train_json_l_dir = os.path.join(base_dir, \"annabell/experiments/data\")\n",
    "nyc_train_json_l_filename = \"nyc_squad_train.jsonl\"\n",
    "nyc_train_json_l_filepath = os.path.join(nyc_train_json_l_dir, nyc_train_json_l_filename)\n",
    "prompt_directory = \"prompts\"\n",
    "prompt_filename = \"declarative_sentences_prompt_JSON.jsonl\"\n",
    "prompt_path = os.path.join(prompt_directory, prompt_filename)\n",
    "response_filename = \"responses.jsonl\"\n",
    "response_filepath = os.path.join(nyc_train_json_l_dir, response_filename)"
   ],
   "id": "14db9117308fb5c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = load_squad_dataset(dataset_path)\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "nyc_train_df = train_df[train_df[\"title\"] == \"New_York_City\"]\n",
    "#add columns to the dataframe for response_question, response_answer, response_declarative_sentence, test_answer\n",
    "nyc_train_df[\"response_question\"] = \"<INSERT RESPONSE QUESTION HERE>\"\n",
    "nyc_train_df[\"response_answer\"] = \"<INSERT RESPONSE ANSWER HERE>\"\n",
    "nyc_train_df[\"response_declarative_sentence\"] = \"<INSERT RESPONSE SENTENCE HERE>\"\n",
    "#drop the context and the title columns\n",
    "nyc_train_df = nyc_train_df.drop(columns=[\"context\", \"title\"])\n",
    "#create an answer column that contains the first answer text if there are any answers, otherwise an empty string\n",
    "nyc_train_df['answer'] = nyc_train_df['answers'].apply(lambda x: x['text'][0] if x['text'] else '')\n",
    "nyc_train_df = nyc_train_df.drop(columns=[\"answers\"])\n",
    "nyc_train_df.reset_index(drop=True, inplace=True)\n",
    "#write the dataframe to a jsonl file\n",
    "nyc_train_df.to_json(nyc_train_json_l_filepath, orient=\"records\", lines=True)\n",
    "print(\"NYC train dataframe written to: \" + nyc_train_json_l_filepath)\n",
    "#for each row in the dataframe create an entry in JSON format.\n",
    "prompt_entries = []\n",
    "for index, row in train_df.iterrows():\n",
    "\tquestion = row[\"question\"]\n",
    "\tanswer = row[\"answers\"][\"text\"][0] if len(row[\"answers\"][\"text\"]) > 0 else \"\"\n",
    "\tprompt_entry = {\n",
    "\t\t\"declarative_sentence\": \"<INSERT DECLARATIVE SENTENCE HERE>\",\n",
    "\t\t\"original_question\": question,\n",
    "\t\t\"original_answer\": answer,\n",
    "\t}\n",
    "\tprompt_entries.append(prompt_entry)\n",
    "#write the prompt entries to a jsonl file\n",
    "with open(prompt_path, \"w\") as prompt_file:\n",
    "\tfor entry in prompt_entries:\n",
    "\t\tprompt_file.write(json.dumps(entry) + \"\\n\")\n",
    "print(\"Prompt written to: \" + prompt_path)"
   ],
   "id": "99eab8dc595f1d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Use the prompt with the Gemini 2.5 Pro model, paste the response into the responses.jsonl file\n",
    "Note that the input file that goes with the prompt had to be split in 2 as Gemini in chrome kept aborting the processing with the full file."
   ],
   "id": "2639d4ee31aa4f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
