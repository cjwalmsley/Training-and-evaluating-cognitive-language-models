{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T05:41:58.434648Z",
     "start_time": "2025-06-12T05:41:57.401180Z"
    }
   },
   "source": "from generate_declarative_sentences import generated_text_from_prompt, load_options_from_config_file",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting the model type and parameters\n",
    "\n",
    "### The table below describes the parameters that can be set to control the output of the model.\n",
    "\n",
    "[github.com/ollama/](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "\n",
    "| Parameter      | Description                                                                                                                                                                                                                                                                                                                                                                | Value Type | Example Usage        |\n",
    "|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|----------------------|\n",
    "| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 2048)                                                                                                                                                                                                                                                                                       | int        | num_ctx 4096         |\n",
    "| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                                                                                                                                              | int        | repeat_last_n 64     |\n",
    "| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                                                                                                                                        | float      | repeat_penalty 1.1   |\n",
    "| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                                                                                                                                        | float      | temperature 0.7      |\n",
    "| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                                                                                                                                          | int        | seed 42              |\n",
    "| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.                                                                                                                                                           | string     | stop \"AI assistant:\" |\n",
    "| num_predict    | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)                                                                                                                                                                                                                                                                               | int        | num_predict 42       |\n",
    "| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                                                                                                                                           | int        | top_k 40             |\n",
    "| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                                                                                                                                    | float      | top_p 0.9            |\n",
    "| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05           |"
   ],
   "id": "74b62976a45b9c7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:42:17.366560Z",
     "start_time": "2025-06-12T05:42:17.361280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = load_options_from_config_file()\n",
    "\n",
    "model_strings = (\n",
    "            \"llama3.2\",\n",
    "            \"llama3\",\n",
    "            \"deepseek-r1:8b\",\n",
    "            \"gemma3:4b\")\n",
    "model_string = model_strings[-1]"
   ],
   "id": "375fff9bd30410d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T05:48:23.215882Z",
     "start_time": "2025-06-13T05:48:07.012073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prompts/prompt_prefix_for_squad\", \"r\") as prompt_prefix_file:\n",
    "    prompt_prefix = prompt_prefix_file.read()\n",
    "\n",
    "with open(\"prompts/sample_qas_from_squad\", \"r\") as prompt_qa_file:\n",
    "    prompt_qas = [line.split(\"\\t\") for line in prompt_qa_file.readlines()]\n",
    "\n",
    "for qa in prompt_qas:\n",
    "    prompt_suffix = \"question: \" + qa[0] + \"\\nanswer: \" + qa[-1]\n",
    "    prompt = prompt_prefix + \"\\n\" + prompt_suffix\n",
    "    response = generated_text_from_prompt(prompt, model_string, options)\n",
    "    print(response)"
   ],
   "id": "b5a1b52a4cc6a6a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to whom did the virgin mary allegedly appear in 1858 in lourdes france?\n",
      "saint Bernadette Soubirous\n",
      "the virgin mary allegedly appeared in 1858 in lourdes france to saint Bernadette Soubirous\n",
      "what is in front of the Notre Dame Main Building?\n",
      "a copper statue of Christ\n",
      "the Notre Dame Main Building is in front of a copper statue of Christ\n",
      "the Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "the Main Building\n",
      "the Basilica of the Sacred heart at Notre Dame is beside the Main Building\n",
      "what is the grotto at notre dame?\n",
      "a Marian place of prayer and reflection\n",
      "the grotto at notre dame is a Marian place of prayer and reflection\n",
      "what sits on top of the Main Building at Notre Dame?\n",
      "a golden statue of the Virgin Mary\n",
      "the Main Building has a golden statue of the Virgin Mary on top of it\n",
      "when did the scholastic magazine of notre dame begin publishing?\n",
      "september 1876\n",
      "the scholastic magazine of notre dame began publishing in september 1876\n",
      "how often is Notre Dame’s the Juggler published?\n",
      "twice\n",
      "Notre Dame’s the Juggler is published twice\n",
      "what is the daily student paper at Notre Dame called?\n",
      "the observer\n",
      "the observer is the daily student paper at Notre Dame\n",
      "how many student news papers are found at Notre Dame?\n",
      "three\n",
      "there are three student news papers found at Notre Dame\n",
      "in what year did the student paper common sense begin publication at notre dame?\n",
      "1987\n",
      "the student paper common sense began publication at notre dame in 1987\n",
      "how many Hispanic people live in the New York metropolitan area?\n",
      "4.8 million\n",
      "4.8 million Hispanic people live in the New York metropolitan area\n",
      "How many acres of land does Gateway Nation Recreation contain?\n",
      "over 26,000\n",
      "gateway nation recreation contains over 26,000 acres of land\n",
      "what is the population of New York's Combined Statistical Area?\n",
      "23.6 million\n",
      "New York’s Combined Statistical Area has a population of 23.6 million\n",
      "in what year were the five borough -s combined into one city?\n",
      "8,491,079\n",
      "the census estimated the population of New York City to be 8,491,079 in 20\n",
      "the Bronx River\n",
      "the Bronx River is the single only freshwater river in NYC\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ba071c70f6bafdfb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
