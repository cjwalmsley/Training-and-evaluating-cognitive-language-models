{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T06:37:18.141982Z",
     "start_time": "2025-06-06T06:37:18.136090Z"
    }
   },
   "source": "from generate_declarative_sentences import generated_text_from_prompt",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting the model type and parameters\n",
    "\n",
    "### The table below describes the parameters that can be set to control the output of the model.\n",
    "\n",
    "[github.com/ollama/](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "\n",
    "| Parameter      | Description                                                                                                                                                                                                                                                                                                                                                                | Value Type | Example Usage        |\n",
    "|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|----------------------|\n",
    "| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 2048)                                                                                                                                                                                                                                                                                       | int        | num_ctx 4096         |\n",
    "| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                                                                                                                                              | int        | repeat_last_n 64     |\n",
    "| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                                                                                                                                        | float      | repeat_penalty 1.1   |\n",
    "| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                                                                                                                                        | float      | temperature 0.7      |\n",
    "| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                                                                                                                                          | int        | seed 42              |\n",
    "| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.                                                                                                                                                           | string     | stop \"AI assistant:\" |\n",
    "| num_predict    | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)                                                                                                                                                                                                                                                                               | int        | num_predict 42       |\n",
    "| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                                                                                                                                           | int        | top_k 40             |\n",
    "| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                                                                                                                                    | float      | top_p 0.9            |\n",
    "| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05           |"
   ],
   "id": "74b62976a45b9c7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T06:42:29.518Z",
     "start_time": "2025-06-06T06:42:29.514744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = {\n",
    "    \"num_ctx\": 4096,\n",
    "    \"repeat_last_n\": 64,\n",
    "    \"repeat_penalty\": 1.5,\n",
    "    \"temperature\": 0,\n",
    "    \"seed\": 42,\n",
    "    \"num_predict\": 50,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.0,\n",
    "    \"min_p\": 0.0}\n",
    "\n",
    "model_strings = (\n",
    "            \"llama3.2\",\n",
    "            \"llama3\",\n",
    "            \"deepseek-r1:8b\",\n",
    "            \"gemma3:4b\")\n",
    "model_string = model_strings[-1]"
   ],
   "id": "375fff9bd30410d0",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T06:42:42.758822Z",
     "start_time": "2025-06-06T06:42:33.457513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prompts/prompt_prefix_for_squad\", \"r\") as prompt_prefix_file:\n",
    "    prompt_prefix = prompt_prefix_file.read()\n",
    "\n",
    "with open(\"prompts/sample_qas_from_squad\", \"r\") as prompt_qa_file:\n",
    "    prompt_qas = [line.split(\"\\t\") for line in prompt_qa_file.readlines()]\n",
    "\n",
    "for qa in prompt_qas:\n",
    "    prompt_suffix = \"question: \" + qa[0] + \"\\nanswer: \" + qa[-1]\n",
    "    prompt = prompt_prefix + \"\\n\" + prompt_suffix\n",
    "    response = generated_text_from_prompt(prompt, model_string, options)\n",
    "    print(response)"
   ],
   "id": "b5a1b52a4cc6a6a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City is 3 square mile -s\n",
      "the Virgin Mary allegedly appeared in 1858 in Lourdes France to Saint Bernadette Soubirous\n",
      "the Notre Dame Main Building is in front of a copper statue of Christ\n",
      "the Basilica of the Sacred heart at Notre Dame is beside the Main Building\n",
      "the Grotto at Notre Dame is a Marian place of prayer and reflection\n",
      "the Main Building sits on top of a golden statue of the Virgin Mary\n",
      "the Scholastic Magazine of Notre dame began publishing in September 1876\n",
      "Notre Dame’s the Juggler is published twice\n",
      "the Observer is the daily student paper at Notre Dame\n",
      "three student news paper -s are found at Notre Dame\n",
      "the student paper Common Sense began publication at Notre Dame in 1987\n",
      "the four million –eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand –thousand eight hundred eighty thousand\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
