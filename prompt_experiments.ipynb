{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T15:37:34.259890Z",
     "start_time": "2025-07-07T15:37:33.674452Z"
    }
   },
   "source": "from generate_declarative_sentences import generated_text_from_prompt, load_options_from_config_file, generated_json_from_prompt",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting the model type and parameters\n",
    "\n",
    "### The table below describes the parameters that can be set to control the output of the model.\n",
    "\n",
    "[github.com/ollama/](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "\n",
    "| Parameter      | Description                                                                                                                                                                                                                                                                                                                                                                | Value Type | Example Usage        |\n",
    "|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|----------------------|\n",
    "| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 2048)                                                                                                                                                                                                                                                                                       | int        | num_ctx 4096         |\n",
    "| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                                                                                                                                              | int        | repeat_last_n 64     |\n",
    "| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                                                                                                                                        | float      | repeat_penalty 1.1   |\n",
    "| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                                                                                                                                        | float      | temperature 0.7      |\n",
    "| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                                                                                                                                          | int        | seed 42              |\n",
    "| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.                                                                                                                                                           | string     | stop \"AI assistant:\" |\n",
    "| num_predict    | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)                                                                                                                                                                                                                                                                               | int        | num_predict 42       |\n",
    "| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                                                                                                                                           | int        | top_k 40             |\n",
    "| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                                                                                                                                    | float      | top_p 0.9            |\n",
    "| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05           |"
   ],
   "id": "74b62976a45b9c7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:37:34.326516Z",
     "start_time": "2025-07-07T15:37:34.322569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = load_options_from_config_file()\n",
    "\n",
    "model_strings = (\n",
    "            \"llama3.2\",\n",
    "            \"llama3\",\n",
    "            \"deepseek-r1:8b\",\n",
    "            \"gemma3:4b\",\n",
    "            \"gemma3:1b\")\n",
    "model_string = model_strings[-1]"
   ],
   "id": "375fff9bd30410d0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:37:41.715249Z",
     "start_time": "2025-07-07T15:37:40.053673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prompts/prompt_sandbox\", \"r\") as prompt_file:\n",
    "    the_prompt = prompt_file.read()\n",
    "\n",
    "print(generated_json_from_prompt(model_string, the_prompt, options))\n"
   ],
   "id": "6536061ba184a0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question='What public figure defended New York in January 2016?' answer='Donald Trump' statement='Donald Trump defended New York in January 2016.'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:38:53.114867Z",
     "start_time": "2025-07-07T15:38:20.467151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prompts/prompt_prefix_for_squad\", \"r\") as prompt_prefix_file:\n",
    "    prompt_prefix = prompt_prefix_file.read()\n",
    "\n",
    "with open(\"prompts/sample_qas_from_squad\", \"r\") as prompt_qa_file:\n",
    "    prompt_qas = [line.split(\"\\t\") for line in prompt_qa_file.readlines()]\n",
    "\n",
    "for qa in prompt_qas:\n",
    "    prompt_suffix = \"question: \" + qa[0] + \"\\nanswer: \" + qa[-1]\n",
    "    prompt = prompt_prefix + \"\\n\" + prompt_suffix\n",
    "    response = generated_json_from_prompt(model_string, prompt, options)\n",
    "    print(response)"
   ],
   "id": "8dd77ed9e8a850d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question='To whom did the Virgin Mary allegedly appear in 1858 in Lourdes, France?' answer='Saint Bernadette Soubirous' statement='The Virgin Mary allegedly appeared to Saint Bernadette Soubirous in 1858 in Lourdes, France.'\n",
      "question='What is in front of the Notre Dame Main Building?' answer='a copper statue of Christ' statement='a copper statue of Christ is in front of the Notre Dame Main Building.'\n",
      "question='The Basilica of the Sacred heart at Notre Dame is beside to which structure?' answer='the Main Building' statement='The Basilica of the Sacred heart at Notre Dame is beside the Main Building.'\n",
      "question='What is the Grotto at Notre Dame?' answer='a Marian place of prayer and reflection' statement='The Grotto at Notre Dame is a Marian place of prayer and reflection.'\n",
      "question='What sits on top of the Main Building at Notre Dame?' answer='a golden statue of the Virgin Mary' statement='a golden statue of the Virgin Mary sits on top of the Main Building at Notre Dame.'\n",
      "question='When did the Scholastic Magazine of Notre dame begin publishing?' answer='September 1876' statement='The Scholastic Magazine of Notre dame began publishing in September 1876.'\n",
      "question='How often is Notre Dame’s the Juggler published?' answer='twice' statement='Notre Dame’s the Juggler is published twice.'\n",
      "question='What is the daily student paper at Notre Dame called?' answer='The Observer' statement='The daily student paper at Notre Dame is called The Observer.'\n",
      "question='How many student news papers are found at Notre Dame?' answer='three' statement='There are three student news papers at Notre Dame.'\n",
      "question='In what year did the student paper Common Sense begin publication at Notre Dame?' answer='1987' statement='The student paper Common Sense began publication at Notre Dame in 1987.'\n",
      "question='How many Hispanic people live in the New York metropolitan area?' answer='4.8 million' statement='The number of Hispanic people living in the New York metropolitan area is 4.8 million.'\n",
      "question='How many acres of land does Gateway Nation Recreation contain?' answer='over 26,000' statement='Gateway Nation Recreation contains over 26,000 acres of land.'\n",
      "question='What is the population of New York’s Combined Statistical Area?' answer='23.6 million' statement='The population of New York’s Combined Statistical Area is 23.6 million.'\n",
      "question='In 2014, what did the census estimate the population of New York City to be?' answer='8,491,079' statement='In 2014, the census estimated the population of New York City to be 8,491,079.'\n",
      "question='The single only freshwater river in NYC is what river?' answer='The Bronx River' statement='The single only freshwater river in NYC is the Bronx River.'\n",
      "question='What public figure defended New York in January 2016?' answer='Donald Trump' statement='Donald Trump defended New York in January 2016.'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:39:43.809685Z",
     "start_time": "2025-07-07T15:39:43.733018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prompts/prompt_prefix_for_squad\", \"r\") as prompt_prefix_file:\n",
    "    prompt_prefix = prompt_prefix_file.read()\n",
    "\n",
    "with open(\"prompts/sample_qas_from_squad\", \"r\") as prompt_qa_file:\n",
    "    prompt_qas = [line.split(\"\\t\") for line in prompt_qa_file.readlines()]\n",
    "\n",
    "for qa in prompt_qas:\n",
    "    prompt_suffix = \"question: \" + qa[0] + \"\\nanswer: \" + qa[-1]\n",
    "    prompt = prompt_prefix + \"\\n\" + prompt_suffix\n",
    "    response = generated_text_from_prompt(model_string, prompt, options)\n",
    "    print(response)"
   ],
   "id": "b5a1b52a4cc6a6a9",
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "invalid JSON schema in format (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mResponseError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m prompt_suffix = \u001B[33m\"\u001B[39m\u001B[33mquestion: \u001B[39m\u001B[33m\"\u001B[39m + qa[\u001B[32m0\u001B[39m] + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33manswer: \u001B[39m\u001B[33m\"\u001B[39m + qa[-\u001B[32m1\u001B[39m]\n\u001B[32m      9\u001B[39m prompt = prompt_prefix + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + prompt_suffix\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m response = \u001B[43mgenerated_text_from_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_string\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/dataset/generate_declarative_sentences.py:27\u001B[39m, in \u001B[36mgenerated_text_from_prompt\u001B[39m\u001B[34m(the_model_string, the_prompt, the_format, the_options)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerated_text_from_prompt\u001B[39m(the_model_string, the_prompt, the_format=\u001B[38;5;28;01mNone\u001B[39;00m, the_options=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     generated_text = \u001B[43mollama\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mthe_model_string\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mthe_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mthe_format\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mthe_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m generated_text.response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/ollama/_client.py:242\u001B[39m, in \u001B[36mClient.generate\u001B[39m\u001B[34m(self, model, prompt, suffix, system, template, context, stream, raw, format, images, options, keep_alive)\u001B[39m\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate\u001B[39m(\n\u001B[32m    217\u001B[39m   \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    218\u001B[39m   model: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    230\u001B[39m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    231\u001B[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001B[32m    232\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    233\u001B[39m \u001B[33;03m  Create a response using the requested model.\u001B[39;00m\n\u001B[32m    234\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    239\u001B[39m \u001B[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m242\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    243\u001B[39m \u001B[43m    \u001B[49m\u001B[43mGenerateResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    244\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mPOST\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/api/generate\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m=\u001B[49m\u001B[43mGenerateRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m      \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m      \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m      \u001B[49m\u001B[43msystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtemplate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtemplate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m      \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m      \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m      \u001B[49m\u001B[43mraw\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m      \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m      \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_copy_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m      \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_none\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/ollama/_client.py:178\u001B[39m, in \u001B[36mClient._request\u001B[39m\u001B[34m(self, cls, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    174\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**part)\n\u001B[32m    176\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m--> \u001B[39m\u001B[32m178\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m.json())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/sandbox/.venv/.venv/lib/python3.12/site-packages/ollama/_client.py:122\u001B[39m, in \u001B[36mClient._request_raw\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m r\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.HTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.ConnectError:\n\u001B[32m    124\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(CONNECTION_ERROR_MESSAGE) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mResponseError\u001B[39m: invalid JSON schema in format (status code: 500)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#the ollama generate fucntion\n",
    "\n",
    "\"\"\"def generate(\n",
    "    self,\n",
    "    model: str = '',\n",
    "    prompt: str = '',\n",
    "    suffix: str = '',\n",
    "    *,\n",
    "    system: str = '',\n",
    "    template: str = '',\n",
    "    context: Optional[Sequence[int]] = None,\n",
    "    stream: Literal[False] = False,\n",
    "    think: Optional[bool] = None,\n",
    "    raw: bool = False,\n",
    "    format: Optional[Union[Literal['', 'json'], JsonSchemaValue]] = None,\n",
    "    images: Optional[Sequence[Union[str, bytes, Image]]] = None,\n",
    "    options: Optional[Union[Mapping[str, Any], Options]] = None,\n",
    "    keep_alive: Optional[Union[float, str]] = None,\n",
    "  )\"\"\""
   ],
   "id": "ba071c70f6bafdfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a290576256d1bd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
